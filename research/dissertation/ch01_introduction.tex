\setcounter{page}{1}    %set correct page here
%%Note: You can only use \section command, you are not allowed, per TTU Graduate School, use
%%\subsection command for higher level subheadings. At most level 2 subheadings are allowed.

\chapter{Introduction}

\bigskip
\setlength{\epigraphwidth}{.6\textwidth}
\begin{epigraphs}
	\qitem{In theory, there is no difference between theory and practice. But, in practice, there is.}%
	{---\textsc{Jan L. A. van de Snepscheut}}
	\centering
\end{epigraphs}
\bigskip

According to the National Institute for Standards and Technology (NIST), "Cyber-Physical Systems (CPS) comprise interacting digital, analog, physical, and human components engineered for function through integrated physics and logic. These systems will provide the foundation of our critical infrastructure, form the basis of emerging and future smart services, and improve our quality of life in many areas."[-@nistcps] We see real-world examples of these systems in nearly every aspect of daily life: traffic flow management systems; protection systems on our electrical power grid; pump, flow, and chemical controls within a water treatment plant to name just a few. Given the pervasive nature of these systems, logic would dictate that these are some of the most well-protected against attack and misuse. Unfortunately, the reality is often quite the opposite. This condition stems, at least in part, from the fact that many of these systems were designed to operate as independent or air-gapped systems yet are increasingly connected to corporate and other networks.[-@Fuehring:1980:GAP:800250.807493] While security professionals have been raising concerns for years, events such as Stuxnet (2010), the compromise of the dam in Rye Brook, New York (2013) and attacks on the Ukrainian power grid (2015) have awakened both the general public and governmental awareness to this issue. A cursory review of current news and reporting reveals a deep-rooted concern for the security and safety of these systems due to the dramatic effect they can have on our daily lives[-@eocps][-@doenist][-@dhscps][-@lightsout].

As a response to the concern for these systems, recent years have seen both government and industry make both significant investments and progress in developing approaches for the security and reliable operation of cyber-physical systems. These investments span a wide range of research including hardware, software, modeling and simulation, and empirical experiments. Many of these efforts have been funded on the premise that improved use of artificial intelligence-derived data analytics (machine learning, deep learning, anomaly detection, etc.) is key to properly securing the cyber-physical systems which comprise our critical infrastructure.

Unfortunately, the majority of these efforts suffer from a common flaw. While much effort is exerted in developing the algorithms and techniques to support a given defensive mechanism, little effort is expended in attempting to defeat said approach. This "honeymoon period" is both expected and valuable as new research areas need time to mature before being attacked. The time has now come to develop a scientifically based critical eye when looking at these defensive techniques and to establish a capability to challenge their assertions in real-world scenarios. Such a capability should be both measured and disciplined in its approach and target the assumptions of both science and implementation.

Making the previous assertion is the easy part - the execution of such is much harder. Questions such as "how should one actually assess the suitability of such a system?" logically follow yet remain largely unanswered. Many papers treat anomaly detection systems as traffic classifiers and then utilize common f-score approaches to quantify their successes.[-@Goss:2017:API:3067695.3076018][-@Zanero:2008:UNI:1413140.1413163] Others[-@Gu:2006:MID:1128817.1128834] take an information theoretic approach which, while valid in their own right, struggle to account for the impact of real-world deployment considerations and configuration settings.

I propose to take a data-driven approach to attack the defenses of cyber-physical systems and more specifically, industrial control system networks. Existing research[-@klof:laskov] has established that, if one can know when an anomaly detection system is being trained, one can poison the training data and thereby affect the definition of "normal" - allowing attacks that would otherwise be caught to succeed. My research aims to measure the degree to which a given system is susceptible to these types of attacks and standard attacks can be made to succeed. Further, I aim to externally measure (or estimate) the bounds of the system's definition of normal to ascertain if a patient attacker (using his own anomaly detection system for the purposes of discovering the likely bounds of the deployed system) can craft attacks such that they are accepted as normal by the protection platform. The output of this effort will be a scoring or measurement system which conveys the degree to which a system is susceptible to these types of attacks and can be utilized to inform the defensive posture of such.

In a fashion not unlike cryptographic systems, it is often discovered that deployed systems exhibit behaviors different than the theoretical models and it is the assumptions or compromises made during the engineering and development that provide the most fertile ground for attack. As such, this research will focus on specific instantiations of the protection methodologies rather than evaluating theoretical or model-based designs. These systems have many "knobs" (configuration settings) which may significantly impact the realized effectiveness of the defense. A simple example is how the administrator adjusts the minimum anomaly score after which alerts are generated. There is an incentive to set this high enough to reduce false-positives yet low enough to not miss actual incidents of concern. One objective of this work is to help quantify the ramifications of changing such a setting. It is important to understand, however, that due to the assessment methodology (black box), the assessment will be unable to differentiate between the ramifications of an administrator-controlled configuration setting and a system-designer algorithm selection. An attacker would not generally be privy to this information and, as such, neither will I.

Based on the initial study of the domain, the primary research problems to be addressed are 1) effective manipulation of network traffic to assess the bounds of the system's definition of "normal" and 2) alteration of existing attacks to attempt to conform to this definition. Work on the first challenge will be rooted in the assumption that the attacker has no visibility into how the protection system is configured (e.g. features used, algorithms applied, feature weights, hyper-parameters, etc.) but is positioned on the network such that he can observe alerts that are generated (black-box testing). One can view this stage as a multi-dimensional parameter sweep that attempts to reverse engineer both the features and the associated weights used in the model. The result does not need to be perfect, but sufficient to inform the second phase. This stage attempts to determine the location of the threshold as illustrated in Figure anomscores. It is expected that the level of effort to develop this interrogation capability will be significant as some algorithms actively adjust and only trigger if the rate of change exceeds a certain threshold relative to the baseline[-@7911887] (rather than simple deltas from the norm).

![Example of anomalies relative to a threshold. The focus of the first stage of this research is to see if an attacker can ascertain the position of the threshold line. The second stage is to see if the standard attacks can be modified in a fashion as to live below that threshold. 

Work on the second problem will focus on modifying the attacks in such a way as to comply (if possible) with the parameters derived during the first step. This will require an understanding of the nuance and intent of the attack and will model the position of a sophisticated attacker. Referencing once again Figure anomscores, the detected anomalies in this example are almost laughably obvious. The real question that should be asked, is how well the system can protect against attacks that are \emph{just barely} different than the norm. Assuming the attack remains successful, an attempt will be made to quantify the relation between the "slack" available via the realized definition of "normal" and the effectiveness of the attack. For example, a configuration that allows a successful attack that slowed the attack from 5 seconds to 5 minutes would receive a worse score than a configuration that required the same attack to now take three days, or reduced the likelihood of success from 90% to 30%.

I do not intend to focus heavily on the numeric methods of expressing this relationship. The approach is certainly important and will be discussed, however, the intent is to express a relative relationship between two configurations within an otherwise identical environment and not to establish a globally-relevant score for a particular system or methodological approach.


This is my test. All electronic devices have non-ideal filters, manufacturing design variances, and a variety of other imperfections that can lead to unintended radiated emissions (URE) from clocked signals, frequency mixing, and signal modulations \cite{Stagner2014, Boroyevich2014, Meynard2011, Prvulovic2017}.  The processing of URE can provide a significant amount of information about the equipment that generated the signals, including equipment type \cite{Wang2012} or its current operating condition \cite{Vuagnoux2009}.  URE characterization research can be broadly separated into the areas of 1) Electromagnetic interference (EMI), 2) Non-intrusive load monitoring (NILM), and 3) Information security (IS). EMI research generally focuses on minimizing URE, NILM on understanding URE, and IS on exploiting URE.  URE appears both in the radiated electromagnetic (EM) spectrum and conducted onto the power infrastructure, with the latter typically being where NILM research is applied to load disaggregation for energy efficiency applications and fault detection for condition-based maintenance applications \cite{Harrold1979, Maughan2010, Timperley2017}.   In addition, information security research focuses on understanding emanations from information processing systems and preventing unauthorized access to private or protected information. 

The majority of NILM research does not actually focus on the unintended emissions from equipment themselves, but rather the load profile and voltage perturbations induced onto the power infrastructure with the operation of electrical equipment.  NILM signal processing can be roughly divided into transient and steady state analysis, with both often requiring more than one measurement point such as current and voltage \cite{Dinesh2016}.  Transient analysis works well with large inductive and machine loads due to the current draw and perturbations generated with large equipment start-up; however, these techniques can be confounded by simultaneous turn-on events \cite{Chang2012}.  Steady-state approaches utilize real and reactive power draw of equipment as well as power line harmonic analysis to detect and classify equipment, but often utilize changes in steady-state operations for triggering which is not applicable to always-on devices and can be confounded with a large number of devices and transients \cite{Laughman2003}.  Fault detection for condition-based maintenance applications, such as \cite{Benbouzid2000}, does utilize unintended emanations and, in addition, \cite{Stagner2014} demonstrates that analysis of URE transients associated with switching frequencies could be used as a potential feature for NILM device characterization.  There is a significant amount of literature on switching frequency emanations, but it generally focuses on development of EMI mitigation strategies, such as clock spreading as shown by \cite{Hardin1994} and \cite{Hsiang-Hui2003}; however \cite{Cooke2016} does present a harmonic analysis of USB charger switching frequencies for NILM applications and \cite{Gulati2014} analyzes EMI signatures of switched-mode power supplies for device identification.  Although a significant amount of NILM research has been applied to determining optimal machine learning classifiers, such as neural networks \cite{Srinivasan2006}, decision trees \cite{Gillis2016}, or support vector machines (SVM) \cite{Duarte2012}, the majority of features are derived from the transient and steady state load characteristics. 

IS, or information leakage, research was first presented in the $1960$s \cite{Zaji2014} and URE characterization is a primary focus.  It has been shown that URE can allow an eavesdropper access to keystrokes \cite{Vuagnoux2009}, television images \cite{Kuhn2013, Enev2011}, and even cryptographic information \cite{Hayashi2013a}.  Targeting specific algorithms, software implementations, or hardware implementations may require a deep understanding of the underlying URE generation mechanism or URE characteristics as shown in \cite{Zaji2014}, although new research has shown generic detection and characterization methods for detecting clocked digital circuitry \cite{Stagner2014} and methods for detecting amplitude and frequency modulations within computer emanations \cite{Prvulovic2017}.  A similar intimate understanding of the design and physical layout of hardware circuitry is also required in the EMI research community, as is demonstrated in the plethora of literature dedicated to reducing EMI of voltage converters \cite{Liu2007}, pulse-width-modulation (PWM) generation \cite{Skibinski1999}, and clocking methods \cite{Zhou2011}. 

Whether detecting and characterizing an electrical kitchen appliance for a NILM application or determining the state of a computer algorithm for an information leakage research effort, URE characterization can be viewed as a typical machine learning classification problem.  Though significant progress has been accomplished in terms of load characterization for NILM applications, no approach has been applied to align URE-based features for a significantly reduced feature space.  DASP and associated transformations are presented as a new method for generating features from URE for NILM and associated URE classification applications.  The DASP methodology aligns signal characteristics that are inherent to electrical circuitry to generate features for subsequent machine learning classifiers.  Harmonic, modulation, and frequency spacing signal dimensions were utilized to demonstrate the performance improvements gained by aligning these signal characteristics into a 2D image which is subsequently summed into a 1-D vector for statistical feature extraction or processed directly by a computer vision machine learner. 

A typical commercial or residential facility contains a large number of electrical devices which all generate URE.  A measurement of the electrical infrastructure would therefore contain the superposition of all URE time domain signals, which could significantly complicate and confound a NILM processor.  In addition to characterizing the URE from a single device, DASP generated images were processed by a convolutional neural network (CNN) image recognition machine learner to demonstrate applicability in multi-device and high noise URE environments.  

CNNs are a deep learning framework specifically developed for object recognition and computer vision applications \cite{Krizhevsky2012} and have achieved significant success in the ImageNet Large Scale Visual Recognition Challenge \cite{Simonyan2014, Szegedy2015, He2016}.  CNNs are scale and shift invariant \cite{Scherer2010} allowing for recognition of objects, shapes, or features regardless of location or translation within an image, which makes them well suited for computer vision applications.  The CNN operates on an image by isolating low level features, such as ``door'', ``windows'', and ``chimney'', that, in combination, are identified as a ``house'' at the higher level learning layers \cite{LeCun2015}.  The vast majority of CNN research is applied to the processing of images, mostly optical \cite{Simonyan2014, Szegedy2015, He2016} or medical \cite{Chen2017, Tajbakhsh2016}, with little research currently being applied to synthetic images originating from scientific data, such as DASP; however recent applications in voice spectral image analysis \cite{Qian2016} and particle physics experimental analysis \cite{Aurisano2016, Racah2016} indicate increasing adoption of CNN learning frameworks in new research areas.  The CNN learning process, along with its scalability, flexible architecture, and shift invariance, make it uniquely suited for processing of single device DASP images and DASP images with multiple superimposed URE signatures in particular.     

\section[Dissertation Outline]{Dissertation Outline}

In Chapter \ref{URE Model Development Chapter} a model for the generation and conduction of unintended emanations is derived from the inherent operation of electronic devices.  To evaluate the performance of the DASP algorithms, time domain captures of URE were collected from commercial electronic devices as outlined in Chapter \ref{URE Data Collection Chapter}.  Chapter \ref{DASP Algorithm Development Chapter} describes the development of the Harmonically Aligned Signal Projection (HASP), Modulation Aligned Signal Projection (MASP), Spectral Correlation Aligned Projection (SCAP), Cross-modulation Aligned Signal Projection (CMASP), and Frequency Aligned Signal Projection (FASP) dimensional alignment algorithms.  Chapters \ref{DASP Feature Extraction Chapter} and \ref{Simulation and Testing Configuration} outline the processes for transforming, scaling, and extracting features from DASP generated images and establishes the testing parameters and procedures for the evaluation of the DASP algorithms using the linear discriminant analysis (LDA), k-nearest neighbor (k-NN), and CNN machine learning methods, as described in Chapter \ref{DASP Device Classification Chapter}.
 

%%%%%  Adding nomenclature 
%
%\nomenclature{PSD}{Power Spectral Density}
%\nomenclature{DASP}{Dimensionally Aligned Signal Projection}
%\nomenclature{CMASP}{Cross-Modulation Aligned Signal Projection}
%\nomenclature{HASP}{Harmonically Aligned Signal Projection}
%\nomenclature{HASP-F}{Harmonically Aligned Signal Projection - Fixed Type}
%\nomenclature{HASP-D}{Harmonically Aligned Signal Projection - Decimation Type}
%\nomenclature{FASP}{Frequency Aligned Signal Projection}
%\nomenclature{MASP}{Modulation Aligned Signal Projection}
%\nomenclature{SCAP}{Spectral Correlation Aligned Projection}
%\nomenclature{URE}{Unintended Radiated Emissions}
%\nomenclature{NILM}{Non-intrusive Load Monitoring}
%\nomenclature{LDA}{Linear Discriminant Analysis}
%\nomenclature{k-NN}{K-Nearest Neighbor}
%\nomenclature{CNN}{Convolutional Neural Network}
%\nomenclature{FFT}{Fast Fourier Transform}
%\nomenclature{EM}{Electro-magnetic}
%\nomenclature{IS}{Information Security}
%\nomenclature{EMI}{Electro-magnetic Interference}
%\nomenclature{SVM}{Support Vector Machines}
%\nomenclature{PWM}{Pulse Width Modulation}
%\nomenclature{USRP}{Universal Software Radio Peripheral}
%\nomenclature{TCXO}{Temperature Controlled Crystal Oscillator}
%\nomenclature{GPS}{Global Positioning System}
%\nomenclature{RF}{Radio-frequency}
%\nomenclature{STFT}{Short-time Fourier Transform}
%\nomenclature{LoG}{Laplacian of Gaussian}
%\nomenclature{TIFF}{Tag Image File Format}
%\nomenclature{PCA}{Principal Component Analysis}
%\nomenclature{QDA}{Quadrature Discriminant Analysis}
%\nomenclature{SGDM}{Stochastic Gradient Descent with Momentum}
%\nomenclature{reLu}{Rectifying Linear Unit}
%\nomenclature{ACC}{Accuracy}
%\nomenclature{TPR}{True Psitive Rate}
%\nomenclature{FPR}{False Positive Rate}
%\nomenclature{TNR}{True Negative Rate}
%\nomenclature{FNR}{False Negative Rate}
%\nomenclature{PR}{Precision}
%\nomenclature{ROC}{Receiver Operation Characteristic}
%\nomenclature{kS/s}{Kilosamples per second}
%\nomenclature{MS/s}{Megasamples per second}
%\nomenclature{ppm}{parts-per-million}
%\nomenclature{MHz}{Megahertz}
%\nomenclature{kHz}{Kilohertz}
%\nomenclature{Hz}{Hertz}
%\nomenclature{dB}{Decibel}
%\nomenclature{NA}{Not applicable}
%\nomenclature{$\ast$}{Convolution}
%\nomenclature{$\sum{}$}{Summation Operator}
%\nomenclature{$\left|S\right|$}{Absolute value of $S$}
%\nomenclature{$\bf{C}_{XX}$}{Autocovariance function}
%\nomenclature{$\nabla$}{Gradient}
%\nomenclature{$\textbf{E}$}{Expected Value}
%\nomenclature{$\mathcal{O}()$}{Order of a Function}

%\section[Non-Intrusive Load Monitoring]{Non-Intrusive Load Monitoring}
%
%\blindtext[1]
%
%\section[Information Leakage]{Information Leakage}
%
%\blindtext[1]
%
%\section[Electromagnetic Interference Suppression]{Electromagnetic Interference Suppression}
%
%\blindtext[1]