# FUDGE: Fuzz Driver Generation at Scale

https://storage.googleapis.com/pub-tools-public-publication-data/pdf/d92eead1d1b76495e7cc148adf249ea50f3b881e.pdf

To fuzz something, you need a fuzz "driver" - a block of code that exercises the code you care about and can pass it inputs.

This process is largely manual... this negatively influences the adoption of fuzzing

Google claims to have found "tens of thousands of security and robustness bugs" via fuzzing.

"FUDGE" (their new system) aims to automate this process (developing fuzz drivers) based on extant client side code.

Note that they claim to have generated "thousands" of new drivers using FUDGE, but then they indicate that they have integrated "over 200" (significant reduction) into continuous fuzzing tools (may be many reasons for this drop-off). They have found "over 150 bugs"... how long have they been running it?

Bug classes include:

- Buffer Overflows
- Use-after-free
- integer overflows
- uninitialized memory

They claim that fuzzing infrastructures have "converged" to using the `LLVMFuzzerTestOneInput` interface between fuzzers and fuzz drivers. Is this true? If so, does it stand to reason that we should understand it more/some/develop some expertise in such?

This `fuzz target` (a concretization of the `LLVMFuzzerTestOneInput` interface) can theoretically be used with various fuzzer engines (libFuzzer, AFL, Hongfuzz, SBF, etc.)

There are some key thoughts about "good" tests/fuzzing inputs on the top of page two:

- crashes should only happen in the case of implementation bugs
- crashes should *not* occur based on violations of a function's preconditions (e.g. read from a file that was not yet opened).
- things should be determinisic
- should be free of side-effects
- should be able to be reliably reproduced


WEBMIN - 



The process consists of the following steps:

- Slicing
  - goal: determine which lines of code are of interest (winnowing)
  - uses the abstract syntax tree (AST) to determine relevant statements
  - narrows doesn to lines that call the target library and may be a `parsing API` (the API receives a byte buffer argument)
- Synthesis
  - goal: take the results of the slicing operation and try to make a runnable bit of code
  - creates multiple candidate fuzz targets for each snippet (can't know in advance the best way to finalize the snippet)
- Evaluation
  - goal: run each of the candidates generated by the slicing operation and evalute them for initial 'fitness'. 
  - Ranking Indicators:
    - should build successfully
    - should run successfully w/o generating a crash (at least for a few seconds?)
    - size of minimized corpus of the target should be larger than some lower threshold
    - larger code coverage is better


## STATUS

- pick up with section 4


## Questions

- what if the client side code is not exhaustive? 
- How does it catch cases that weren't supported by the clients?
- while this may be helpful, does it rationally catch the edge cases?
- does it's lack of human intuition represent a fault/weakness?


## To look up

- What is `ClangMR`? (analysis and code synthesis)
  - does the `MR` stand for `map reduce`?
- Flume (a C++ implementation of Google's map-reduce)



## RANDOM THOUGHTS

- Can we use deep learning / optimization techniques (think game play approaches) to solving bug discovery or exploitation paths?

- Can we run our own "ClusterFuzz" or "OSS-Fuzz" instances? What would be the benefit/goal/aspirations of such?


## Discussion Thoughts

- spend time with the libfuzzer tutorial

- look at nm

