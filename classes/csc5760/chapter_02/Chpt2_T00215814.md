## Rob Gillen, T00215814 CSC5760, Chapter 2

### Problem 2.1
When we were discussing floating point addition, we made the simplifying assumption that each of the functional units
took the same amount of time. Suppose that fetch and store each take 2 nanoseconds and the remaining operations each
take 1 nanosecond.

a. how long does a floating point addition take with these assumptions?

__Rather than `7 ns` it is `9 ns`__

b. How long will an unpipelined addtion of 1,000 pairs of floats take with these assumptions

__Approximately `9,000 ns` or `9 us`__

c. How long will a pipelined addition of 1,000 pairs of floats take with these assumptions?

__This can be modeled by the following equation: __

$$x = 7 + 2*n$$

__Where $x$ is the total number of nanoseconds and $n$ is the number of floats. Therefore, the total time required to
calculate 1,000 pairs of floats given the updated assumptions is `2,007 ns` or approximately `2 us`.__

\pagebreak

## Rob Gillen, T00215814


d. The time required for fetch and store may vary considerable if the operands/results are stored in different levels of
the memory hierarchy. Suppose that a fetch from a level 1 cache takes two nanoseconds, while a fetch from a level 2
cache takes five nanoseconds, and a fetch from main memory takes fifty nanoseconds. What happens to the pipeline when
there is a level 1 cache miss on a fetch of one of the operands? What happens when there is a level 2 miss?

__The overall efficiency of the pipeline falls apart and, at many points, the pipeline is stalled. Given the assertion
that a pipeline will operate at rate of the slowest stage, and single addition operation will take `9 ns` if accessing via
L1 cache, an L1 miss will result in a per-operation time of `12 ns` and an L2 miss results in a per-operation time of
`57 ns`. The worst case for these scenarios are as follows:__

$$L1_\textrm{miss} = 7 + 5*n$$

$$L2_\textrm{miss} = 7 + 50*n$$

__Resulting in a total time of `5,007 ns` (`5 us`) for all L1 misses and `50,007 ns` (`50 us`) for all L2 misses.__


\pagebreak

## Rob Gillen, T00215814

### Problem 2.4

In Table 2.2, virtual addresses consist of a byte offset of 12 bits and a virtual page number of 20 bits. How many pages
can a program have if it's run on a system with this page size and this virtual address size?

$$x = 2^{20} * 2^{12} = 2^{32} = 4,294,967,296$$


\pagebreak

## Rob Gillen, T00215814

### Problem 2.8

Explain why the performance of a hardware multithreaded processing core might degrade if it had large caches and it ran
many threads.

__The purpose of hardware multi-threading is to identify "stalled" processes (i/o bound or otherwise) and to run other
threads during the time that the system is waiting for the stalled processes to be ready (e.g. time slicing). The
problem with doing this with too many threads if the system has a very large cache, is that moving things in and out of
cache takes time. If thread $T_1$ has filled the cache but then stalled and thread $T_2$ is then time-sliced in and
subsequently loads up the cache with its data and then thread $T_3$ etc... by the time thread $T_1$ comes back in, all
of its previously-cached values may have been expelled resulting in cache misses for each and every access which could
result in a significant degredation of overall performance.__ 

\pagebreak

## Rob Gillen, T00215814

### Problem 2.15

a. Suppose a shared-memory system uses snooping cache coherence and write-back caches. Also suppose that core 0 has the
varible `x` in its cache, and it executes the assignment `x = 5`. Finally suppose that core 1 doesn't have `x` in its
cache, and after core 0's update to `x`, core 1 tries to execute `y = x`. What value will be assigned to `y`? Why?

__`y` will obtain the original value of `x` and _not_ the value `5` (as core 0 sees it). This is due to the fact that core 0
is using write-back caching which means that the in-memory version of `x` only gets updated when core 0 evicts it from
its cache and there is no indication in the problem statement that this has occurred.__

b. Suppose that the shared-memory system in the previous part uses a directory-based protocol. What value will be
assigned to `y`? Why?

__Unfortunately, it appears that `y` will still receive the value originally stored in memory and not the "correct"
value of `5` (from the vantage point of core 0). Directory-based coherence doesn't help in this situation as it appears
that caches are notified only when they already have the cache line of interest in their caches prior to modification.
There is no indication that a line is marked as "dirty" such that an otherwise "clean" cache, upon inserting a cache
line for the first time (e.g. core 1's first access of `x`) is notified that a change has happened previously.__


c. Can you suggest how any problems you found in the first two parts might be solved?

__In the snooping scenario, a write-through caching mechansim would "solve" the problem - assuming, of course, that the
write-through operation completes prior to core 1's read of `x`.__

__In the directory-based approach, a "solution" would be if a new participant in a directory is notified (upon
enrollment) of any prior changes that have not been flushed. This would indicate to core 1 that the value it just pulled
in from memory is incorrect and possibly (if the directory contained said information) it could update its local cached
value based on the stil-to-be-written-back value of `5`.__

\pagebreak

## Rob Gillen, T00215814

### Problem 2.20

Is a program that obtains linear speedup strongly scalable? Explain your answer.

__*Strongly Scalable* is defined as being able to maintain the same efficiency by increasing the thread/core count while
keeping the problem size the same.__

__*Weakly Scalable* is defined as being maintaining the same efficiency by increasing the problem size in proportion to
the increase in thread/core count.__

__*Efficiency* is defined as the value of the speedup introduced by using multiple cores/threads divided by the number
of cores/threads and is expressed by the following equations:__

$$S = T_{serial}/T_{parallel}$$

$$E = S/P$$ 

__*Linear Speedup* is defined as having the time required to execute the parallel version be exactly the time of the
serial version divided by the number of parallelization levels (threads/cores) and is expressed by the following
equation:__

$$T_{parallel} = T_{serial}/p$$

__Therefore, given the definitions above, *linear speedup* is defined by an efficiency of 1 and the maintaining of such is
the definition of *strongly scalable* so yes, a program that exhibits linear speedup can be said to be strongly
scalable.__ 

