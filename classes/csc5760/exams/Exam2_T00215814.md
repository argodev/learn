---
title: CSC5760 Exam 2
author: Rob Gillen
header-includes:
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
    - \fancyhead[RO,RE]{Exam 2}
    - \fancyhead[LO,LE]{Rob Gillen, T00215814}

---

### 1. Which of the following is an example of point-to-point communication in MPI?
_A. MPI_Send_


### 2. Which of the following is an example of collective communication in MPI?
_B. MPI_Reduce_


### 3. The multicore CPU is an example of which of the following?
_D. MIMD (Multiple Instruction, Multiple Data)_


### 4. The GPU is an example of which of the following?
_B. SIMD (Single Instruction, Multiple Data)_


### 5. As described by the book, local variables in MPI programs are what?
_C. Significant only on the process using them_


### 6. What is deadlock? Give an example of this may occur in an MPI program with two or more communicating processes (via `MPI_Send`) and describe how it can be avoided.
If multiple nodes utilize a blocking version of `MPI_Send` and introduce a dependency loop (e.g. `r0` sends to `r1` which is simultaneously sending to `r0`) deadlock can result. In this scenario, both nodes' `send` is followed by a receive, but because they are blocking on the send, and both are waiting for the other to receive, before receiving themselves, deadlock occurs and the system will wait indefinitely.

Avoidance could occur in many ways. One approach is to use non-blocking methods (e.g. `MPI_Isend`, `MPI_Ibsend`, `MPI_Sendrecv`). Another way is to use intelligent scheduling of your communication links (e.g. odd/even nodes, etc.).


### 7. Suppose `comm_sz=4` and suppose that `x` is a vector with `n=14` components

#### a. How would the components of `x` be distributed among the processes in a program that used a block distribution?

* __Rank 0:__ values of `n` at indicies 0, 1, 2, 3
* __Rank 1:__ values of `n` at indicies 4, 5, 6, 7
* __Rank 2:__ values of `n` at indicies 8, 9, 10, 11
* __Rank 3:__ values of `n` at indicies 12, 13

#### b. How would the components of `x` be distributed amont the processes in a program that used cyclic distribution?

* __Rank 0:__ values of `n` at indicies 0, 4, 8, 12
* __Rank 1:__ values of `n` at indicies 1, 5, 9, 13
* __Rank 2:__ values of `n` at indicies 2, 6, 10
* __Rank 3:__ values of `n` at indicies 3, 7, 11


#### c. How would the components of `x` be distributed among the processes in a program that used a block-cyclic distribution with `blocksize b=2`?

* __Rank 0:__ values of `n` at indicies 0, 1, 8, 9
* __Rank 1:__ values of `n` at indicies 2, 3, 10, 11
* __Rank 2:__ values of `n` at indicies 4, 5, 12, 13
* __Rank 3:__ values of `n` at indicies 6, 7, 



### 8. Fully explain collective versus point-to-point communications.
Most of (if not all) of the methods expressed in collective communications could be implemented in a point-to-point fashion. It is mildly helpful (although not entirely accurate) to view the collective communication methods as syntactic sugar on top of the point-to-point methods.

Point-to-point methods are exactly what they sound like... one node (rank) sends a message to one other node. Point-to-point methods could be used to communicate to all nodes (similar to a broadcast) by initiatiating a series of point-to-point messages from the originator to each of the other nodes.

Collective communications provides helper methods to make it easier (and often more efficient) to send messages to all (or some large group) of nodes. In the example of broadcast, the developer simply needs to express the data to be sent, who owns it, and where it should go (all in one line of code), and the communication happens. Similar simplicity is available in MPI_Scatter (sending a portion of data to each of `p` processors in `P`). MPI_Reduce is a strong example of wrapping a significant amount of complexity into a single line of code that not only makes the developer's job easier, but also has some canned optimizations (best practices) for how that reduction occurs.


### 9. Regarding the MPI derived data type; what is it, where is it useful, and why would you use it?
Derived data types are essentially a means of sending a struct, or something similar to "packing" a message in Python... it allows the programmer to bundle a handful of values together into a single message for transport between nodes. This allows there to be fewer messages sent at the cost of slightly larger messages (this trade-off is a tuning parameter). Often, particularly for a handful of small items, bundling them together into a single larger method can be significantly more efficient with respect to overall runtime.


### 10. Describe the differences and possible use cases for MPI, Pthreads, OpenMP, and CUDA
All of these are means of parallelizing applications. MPI is designed for distributed memory platforms whereas the other three all assume a level of shared memory. Because of the shared memory properties, the latter three are usually used as node-local parallelism whereas MPI is used to spread across a collection of nodes. In some scenarios, you will see a combination of MPI and one of the other approaches such that MPI provides the cross-node parallelism and the others provide parallelism within each of the nodes. 

The differences between Pthreads and OpenMP are smaller - both are used for node-local parallelism (shared memory). PThreads tends to provide a finer-grain level of control whereas OpenMP is more accessible and quite easy to utilize (OpenMP is usually implemented on top of PThreads). 

CUDA is targeted specifically at nVidia GPUs and shines for SIMD problems. Further, they shine wherein the data that needs to be moved between the CPU and the GPU (and back) is comparatively small related to the amount of computation to be performed on the GPU (otherwise the overhead becomse too costly).